{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feeb1f5b-2c03-4f27-b1f3-a6ff7f96c3fe",
   "metadata": {},
   "source": [
    "# Возрождение Собора Парижской Богоматери\n",
    "\n",
    "На этом практическом занятии мы отправимся в цифровую экспедицию, чтобы собрать Собор Парижской Богоматери заново, используя синтез современных вычислительных технологий. Мы будем:\n",
    "\n",
    "- Обрабатывать изображения из набора данных Phototour\n",
    "- Моделировать 3D-реконструкцию для создания модели облака точек\n",
    "- Применять теорию вероятностей для представления неопределенности в измерениях\n",
    "- Используйте простую модель машинного обучения для прогнозирования недостающих деталей\n",
    "\n",
    "Повествование отражает нашу историю — приключение, в котором математика объединяет различные области в последовательную реставрацию утраченного шедевра.\n",
    "\n",
    "**ОБРАТИТЕ ВНИМАНИЕ**: все нечётные (1,3,5,7,9,11) задания являются обязательными, а все чётные (2,4,6,8,10) - дополнительными. Приступайте к выполнению дополнительных заданий после того, как выполните все обязательные. (Выполнение всех ячеек с кодом также является обязательным.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528da60",
   "metadata": {},
   "source": [
    "**Цифровая экспедиция: Восстановление Нотр-Дама**\n",
    "\n",
    "* **1 Нотр-Дам-де-Пари: Свидетельство истории**\n",
    "    Собор Парижской Богоматери, известный также как Нотр-Дам-де-Пари, является одним из самых узнаваемых символов Парижа и Франции. Его строительство началось в XII веке и продолжалось на протяжении нескольких столетий, что делает его ярким примером французской готической архитектуры. Собор имеет огромное историческое и культурное значение, являясь местом проведения коронаций, свадеб и других важных событий в истории Франции. Трагический пожар, произошедший в апреле 2019 года, нанес собору значительный ущерб, вызвав волну сочувствия и стремление к его восстановлению и сохранению для будущих поколений.\n",
    "    * **Связь со сниппетами:** Сниппеты подчеркивают важность сохранения наследия Нотр-Дама перед лицом катастрофы. \n",
    "    * **Понимание:** Изучение Нотр-Дама предоставляет убедительный реальный контекст для применения вычислительной математики к значимому объекту культурного наследия.\n",
    "\n",
    "* **2 Искусство и наука фотограмметрии**\n",
    "    Фотограмметрия представляет собой научный метод, позволяющий определять форму, размеры и положение объектов в пространстве по их фотографическим изображениям. Процесс включает в себя получение множества перекрывающихся изображений объекта с разных точек зрения. Затем с помощью специализированного программного обеспечения на этих изображениях идентифицируются общие точки, и на основе их смещения относительно друг друга вычисляется трехмерная структура объекта. Фотограмметрия широко используется для создания 3D-моделей зданий, ландшафтов, артефактов и других объектов.\n",
    "    * **Связь со сниппетами:** Сниппеты раздела 2 явно описывают фотограмметрию как ключевую технологию, использованную для оцифровки и реконструкции Нотр-Дама.\n",
    "    * **Понимание 1:** Фотограмметрия преобразует набор 2D-изображений в ценные 3D-пространственные данные.\n",
    "    * **Понимание 2:** Точность полученной 3D-модели зависит от таких факторов, как качество изображений, степень их перекрытия и калибровка.\n",
    "\n",
    "* **3 Восстановление утраченного наследия: Вычислительный подход**\n",
    "    Представьте себе команду молодых исследователей, которые, используя архивные фотографии разрушенного шедевра, пытаются воссоздать его цифровую модель. Эти фотографии, полученные с помощью фотограмметрии, служат основой для их работы. Используя мощные вычислительные инструменты и математические методы, команда обрабатывает изображения, создает 3D-модель и анализирует ее геометрические свойства. В этом процессе численное интегрирование может стать ценным инструментом для извлечения информации об объеме, площади поверхности и других характеристик реконструируемого объекта. Этот сценарий демонстрирует, как вычислительные методы, включая численное интегрирование, играют важную роль в сохранении и восстановлении культурного наследия."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749a760-1f12-4304-89cb-6a5941944432",
   "metadata": {},
   "source": [
    "## Начало работы\n",
    "\n",
    "Прежде чем приступить к работе, пожалуйста, убедитесь, что вы загрузили набор данных (или соответствующее подмножество) [отсюда](https://phototour.cs.washington.edu/datasets/). После загрузки обновите пути в ячейках ниже соответствующим образом.\n",
    "\n",
    "Также установите необходимые библиотеки, если вы этого еще не сделали. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy opencv-python matplotlib open3d scikit-learn scikit-image -quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e1238-34b3-45ae-9a8b-c9d3b984a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# Open3D for visualization and 3D processing\n",
    "import open3d as o3d\n",
    "\n",
    "# For additional image processing\n",
    "from skimage import filters, feature\n",
    "\n",
    "# For machine learning demonstration\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450868f-f91b-4f2a-bdcc-0910d39dc946",
   "metadata": {},
   "source": [
    "## Шаг 1: Обработка изображений\n",
    "\n",
    "Мы начинаем с выбора образца изображения Собора Парижской Богоматери из набора данных. В этой ячейке мы:\n",
    "\n",
    "- Считываем изображение\n",
    "- Преобразуем его в оттенки серого\n",
    "- Применяем определение границ для выделения структурных особенностей\n",
    "\n",
    "Измените переменную `relpath` так, чтобы она указывала на вашу локальную копию изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "# Replace with the actual image file path after downloading the dataset\n",
    "curr_dir = os.path.abspath(os.path.curdir)\n",
    "relpath = os.path.relpath(\"NotreDame/NotreDame/images/10190279@N06_1027171096.jpg\", curr_dir)\n",
    "print(\"Sample image path:\", os.path.dirname(relpath))\n",
    "print(os.path.lexists(os.path.dirname(relpath)))\n",
    "\n",
    "files = os.listdir(os.path.dirname(relpath))\n",
    "print(files[:5])\n",
    "\n",
    "relpath = os.path.join(os.path.dirname(relpath), files[randrange(len(files))])\n",
    "\n",
    "# Check if the file exists before attempting to read it\n",
    "if os.path.exists(relpath):\n",
    "    img = cv2.imread(relpath)\n",
    "    if img is None:\n",
    "        print(\"Failed to read the image. Please ensure the file is a valid image.\")\n",
    "else:\n",
    "    print(f\"File not found at path: {relpath}. Please check the file path and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1735782b",
   "metadata": {},
   "source": [
    "На практике чаще всего для выделения границ изображения можно использовать готовый оптимизированный библиотечный метод, который дополнительно сделает очистку от шума и много чего [ещё](https://gregorkovalcik.github.io/opencv_contrib/tutorial_py_canny.html).\n",
    "\n",
    "В данном случае важно правильно подобрать значения аргументов функции cv2.Canny.\n",
    "\n",
    "На самом деле, внутри своей реализации метод cv2.Canny также делает интегрирование, но более специфическим для изображений способом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7518c-e3fc-4fc3-8c02-fd565a4bc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Use Canny edge detection\n",
    "edges = cv2.Canny(gray, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce706a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original, grayscale, and edge-detected images\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.title('Grayscale')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.title('Edges')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27d8c3",
   "metadata": {},
   "source": [
    "Попробуем теперь применить собственные методы интегрирования для нахождения границ на изображении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b69d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_integral_trap(gray_image):\n",
    "    \"\"\"\n",
    "    Compute an \"integral image\" approximation for a grayscale image using the cumulative trapezoidal rule.\n",
    "    The integration is computed first along the rows and then along the columns.\n",
    "    \n",
    "    Parameters:\n",
    "      gray_image : 2D numpy array representing the grayscale image.\n",
    "      \n",
    "    Returns:\n",
    "      integral_img : 2D numpy array with the cumulative trapezoidal integration.\n",
    "    \"\"\"\n",
    "    # Integrate along rows (vertical integration); setting initial=0 ensures the output is the same shape.\n",
    "    int_rows = cumulative_trapezoid(gray_image, dx=1, axis=0, initial=0)\n",
    "    # Now integrate the result along columns (horizontal integration)\n",
    "    integral_img = cumulative_trapezoid(int_rows, dx=1, axis=1, initial=0)\n",
    "    \n",
    "    return integral_img\n",
    "\n",
    "\n",
    "def compute_integral(image):\n",
    "    \"\"\"\n",
    "    Compute the integral (summed-area) image for each color channel.\n",
    "    The output 'integral' has shape (H+1, W+1, C) where the extra row and\n",
    "    column (filled with zeros) allow rapid rectangle sum computation.\n",
    "    \"\"\"\n",
    "    H, W, C = image.shape\n",
    "    # Create a padded integral image with zeros in the first row and column.\n",
    "    integral = np.zeros((H + 1, W + 1, C), dtype=np.float64)\n",
    "    for c in range(C):\n",
    "        # For each channel, compute cumulative sums along rows and columns.\n",
    "        integral[1:, 1:, c] = np.cumsum(np.cumsum(image[:, :, c], axis=0), axis=1)\n",
    "    return integral\n",
    "\n",
    "def sum_region(integral, top, left, height, width):\n",
    "    \"\"\"\n",
    "    Compute the sum over a rectangular region in the original image using its integral image.\n",
    "    \n",
    "    Parameters:\n",
    "      integral - the padded integral image of shape (H+1, W+1, C)\n",
    "      top, left - the top-left coordinates (in the original image coordinate system)\n",
    "      height, width - size of the rectangular region\n",
    "      \n",
    "    Returns:\n",
    "      A 1D array (for each channel) with the summed intensity.\n",
    "    \n",
    "    The region covered in the original image is:\n",
    "      rows:    top       to top+height-1\n",
    "      columns: left      to left+width-1\n",
    "    \"\"\"\n",
    "    # Because the integral image is padded, the sum is calculated as:\n",
    "    # S = I[bottom, right] - I[top, right] - I[bottom, left] + I[top, left]\n",
    "    bottom = min(top + height, integral.shape[0] - 1)\n",
    "    right = min(left + width, integral.shape[1] - 1)\n",
    "    return (integral[bottom, right, :] -\n",
    "            integral[top, right, :] -\n",
    "            integral[bottom, left, :] +\n",
    "            integral[top, left, :])\n",
    "\n",
    "def vertical_edge_detection(image, filter_width, filter_height, integration_method):\n",
    "    \"\"\"\n",
    "    Detect vertical edges in a color image using a box filter whose left and right\n",
    "    halves are summed separately (using the integral image) and then differenced.\n",
    "    \n",
    "    Parameters:\n",
    "      image        - Input color image (H, W, 3)\n",
    "      filter_width - Total width of the sliding filter window\n",
    "      filter_height- Height of the filter window\n",
    "      \n",
    "    Returns:\n",
    "      edge_map - A 2D edge response image computed over all valid window positions.\n",
    "    \"\"\"\n",
    "    H, W, C = image.shape\n",
    "    integral = integration_method(image)\n",
    "    \n",
    "    # The output edge response will be computed only where a full window fits.\n",
    "    out_height = H - filter_height + 1\n",
    "    out_width = W - filter_width + 1\n",
    "    edge_map = np.zeros((out_height, out_width), dtype=np.float64)\n",
    "    \n",
    "    # Loop over every position (top-left corner) where the filter fits.\n",
    "    for y in range(out_height):\n",
    "        for x in range(out_width):\n",
    "            # Calculate the sum for the left half of the window.\n",
    "            left_sum = sum_region(integral, y, x, filter_height, filter_width // 2)\n",
    "            # Calculate the sum for the right half of the window.\n",
    "            right_sum = sum_region(integral, y, x + filter_width // 2, filter_height,\n",
    "                                   filter_width - filter_width // 2)\n",
    "            # Compute the difference per channel.\n",
    "            diff = right_sum - left_sum\n",
    "            # Combine responses from all channels (Euclidean norm).\n",
    "            magnitude = np.sqrt(np.sum(diff ** 2))\n",
    "            edge_map[y, x] = magnitude\n",
    "\n",
    "    # Normalize the edge map to the range [0, 255] for visualization.\n",
    "    edge_map = edge_map - edge_map.min()\n",
    "    if edge_map.max() > 0:\n",
    "        edge_map = edge_map / edge_map.max() * 255\n",
    "    return edge_map.astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "# For demonstration, load a grayscale image.\n",
    "img = cv2.imread(relpath, cv2.IMREAD_GRAYSCALE)\n",
    "# if img is None:\n",
    "#     print(\"Error: Image not found. Please ensure 'image.jpg' is available.\")\n",
    "#     exit(1)\n",
    "\n",
    "# integral_img_trap = compute_integral_trap(img)\n",
    "\n",
    "# Also compute the exact integral image using np.cumsum for comparison.\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "# Set filter dimensions. Adjust these based on image resolution and desired sensitivity.\n",
    "filter_width = 20\n",
    "filter_height = 20\n",
    "\n",
    "# Compute the vertical edge map.\n",
    "edge_map_cumsum = vertical_edge_detection(img_rgb, filter_width, filter_height, compute_integral)\n",
    "edge_map_trap = vertical_edge_detection(img_rgb, filter_width, filter_height, compute_integral_trap)\n",
    "\n",
    "\n",
    "# integral_img_exact = np.cumsum(np.cumsum(img, axis=0), axis=1)\n",
    "\n",
    "# For visualization, let's see the differences.\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(\"Original Grayscale Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(edge_map_cumsum, cmap='viridis')\n",
    "plt.title(\"Exact Integral Image (cumsum)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(edge_map_trap, cmap='viridis')\n",
    "plt.title(\"Trapezoidal Integral Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1756be4",
   "metadata": {},
   "source": [
    "### Анализ. Задание 1.\n",
    "\n",
    "Сделайте вывод о применении методов интегрирования для нахождении границ объектов на изображениях. Как нужно дополнить методы интегрирования, чтобы получить лучший результат?\n",
    "\n",
    "*ВАШ КОММЕНТАРИЙ ЗДЕСЬ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код. Задание 2*. \n",
    "# Выполняйте это задание только после того, как выполните все остальные задания\n",
    "# Попробуйте реализовать предлагаемый Вами подход здесь\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2a07f4-5e63-4544-a09a-723ccb0436c1",
   "metadata": {},
   "source": [
    "## Step 2: 3D Reconstruction\n",
    "\n",
    "For 3D reconstruction, a full-scale Structure-from-Motion (SfM) pipeline would typically involve multiple images and feature matching (using tools such as COLMAP or OpenMVG). For illustration, we simulate a reconstructed point cloud using Open3D. \n",
    "\n",
    "In practice, you’d replace this simulation with a true reconstruction process that converts your image data to a point cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f7d67",
   "metadata": {},
   "source": [
    "3D реконструкция объектов по их изображениям - это частный случай фотограмметрии. Фотограмметрия используется и в архитектуре, для восстановления изображений зданий по их фотографиям.\n",
    "\n",
    "Вдохновиться примером фотограмметрии собора Нотр-Дам-де-Пари можно [тут](https://www.reddit.com/r/photogrammetry/comments/1hhpj47/my_photogrammetry_scan_of_notredame_de_paris/). (Автор сам делал фотографии с земли, после чего строил карту высот по точкам и в конце накладывал текстуры.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0be6b",
   "metadata": {},
   "source": [
    "Исходный набор данных уже содержит предобработанный файл с точками, полученными по всем точкам всех изображений с учётом места съемки. Файл записан в формате [Bundler](http://phototour.cs.washington.edu/bundler/bundler-v0.3-manual.html#S6). Приведённый в следующей ячейке код читает данные этого файла.\n",
    "\n",
    "Данные для такого файла рассчитываются при помощи многих методов, включая стерео-зрение, которое мы уже рассматривали на практическом занятии 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b839ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bundle_file(filename):\n",
    "    \"\"\"\n",
    "    Parse a Bundler output file and return lists of camera parameters and 3D points.\n",
    "    \n",
    "    Returns:\n",
    "        cameras (list): Each entry is a dict with keys:\n",
    "                        'focal' : focal length,\n",
    "                        'k1' and 'k2' : radial distortion coefficients,\n",
    "                        'R'     : 3x3 rotation matrix (numpy array),\n",
    "                        't'     : translation vector (numpy array),\n",
    "                        'C'     : computed camera center (numpy array)\n",
    "        points (list): Each entry is a dict with keys:\n",
    "                        'coordinates' : (X, Y, Z) as a numpy array,\n",
    "                        'color'       : (R, G, B) as a numpy array,\n",
    "                        'views'       : list of view observations (each is a dict with camera index,\n",
    "                                        feature index and image coordinates (x, y))\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        # Read and validate header\n",
    "        header = f.readline().strip()\n",
    "        if not header.startswith(\"# Bundle file\"):\n",
    "            raise ValueError(\"The file does not appear to be in Bundler format.\")\n",
    "        \n",
    "        # Read the number of cameras and points\n",
    "        line = f.readline().strip()\n",
    "        num_cameras, num_points = map(int, line.split())\n",
    "        \n",
    "        cameras = []\n",
    "        for i in range(num_cameras):\n",
    "            # Read camera intrinsics: focal length and distortion coefficients.\n",
    "            parts = f.readline().split()\n",
    "            focal = float(parts[0])\n",
    "            k1 = float(parts[1])\n",
    "            k2 = float(parts[2])\n",
    "            \n",
    "            # Read the 3x3 rotation matrix (row by row)\n",
    "            R_rows = []\n",
    "            for _ in range(3):\n",
    "                row = list(map(float, f.readline().split()))\n",
    "                R_rows.append(row)\n",
    "            R = np.array(R_rows)\n",
    "            \n",
    "            # Read the translation vector\n",
    "            t = np.array(list(map(float, f.readline().split())))\n",
    "            \n",
    "            # Compute the camera center: C = - R^T * t.\n",
    "            # (Bundler output is such that a 3D point in camera space is given by: X_camera = R * X_world + t.)\n",
    "            C = -np.dot(R.T, t)\n",
    "            \n",
    "            cameras.append({\n",
    "                'focal': focal,\n",
    "                'k1': k1,\n",
    "                'k2': k2,\n",
    "                'R': R,\n",
    "                't': t,\n",
    "                'C': C\n",
    "            })\n",
    "        \n",
    "        points = []\n",
    "        for i in range(num_points):\n",
    "            # Read the 3D point coordinates\n",
    "            coords = np.array(list(map(float, f.readline().split())))\n",
    "            \n",
    "            # Read the point color (RGB)\n",
    "            color = np.array(list(map(int, f.readline().split())))\n",
    "            \n",
    "            # Read the view list: starts with the number of observations followed by groups of 4 values.\n",
    "            view_data = f.readline().split()\n",
    "            num_views = int(view_data[0])\n",
    "            views = []\n",
    "            index = 1  # starting index for view tuples in view_data\n",
    "            for _ in range(num_views):\n",
    "                cam_idx = int(view_data[index])\n",
    "                feat_idx = int(view_data[index + 1])\n",
    "                x = float(view_data[index + 2])\n",
    "                y = float(view_data[index + 3])\n",
    "                views.append({\n",
    "                    'camera_index': cam_idx,\n",
    "                    'feature_index': feat_idx,\n",
    "                    'x': x,\n",
    "                    'y': y\n",
    "                })\n",
    "                index += 4\n",
    "                \n",
    "            points.append({\n",
    "                'coordinates': coords,\n",
    "                'color': color,\n",
    "                'views': views\n",
    "            })\n",
    "    \n",
    "    return cameras, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d8f4d",
   "metadata": {},
   "source": [
    "Построим визуализацию точек, полученных из всех изображений в файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e714a8d-d7c2-4eef-aa04-d700c8a19979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we simulate a point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "# Load points from the file \"NotreDame\\notredame.out\"\n",
    "curr_dir = os.path.abspath(os.path.curdir)\n",
    "relpath = os.path.join(curr_dir, \"NotreDame\", \"NotreDame\", \"notredame.out\")\n",
    "\n",
    "print(\"Points file path:\", relpath)\n",
    "\n",
    "cameras, points = load_bundle_file(relpath)\n",
    "point_coordinates = [point['coordinates'] for point in points]\n",
    "\n",
    "\n",
    "pcd.points = o3d.utility.Vector3dVector(np.array(point_coordinates))\n",
    "\n",
    "# Visualize the point cloud\n",
    "o3d.visualization.draw_geometries([pcd])\n",
    "# Save the point cloud to a file\n",
    "o3d.io.write_point_cloud(\"notredame.ply\", pcd)\n",
    "\n",
    "\n",
    "print(\"Simulated 3D point cloud generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08316fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's visualize with matplotlib\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x_points = np.array(point_coordinates)[:, 0]\n",
    "y_points = np.array(point_coordinates)[:, 1]\n",
    "z_points = np.array(point_coordinates)[:, 2]\n",
    "ax.scatter(x_points, y_points, z_points, c='b', marker='o')\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_title('3D Point Cloud Visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eeba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.integrate import dblquad\n",
    "\n",
    "# Load a point cloud\n",
    "pcd = o3d.io.read_point_cloud(\"notredame.ply\")\n",
    "points = np.asarray(pcd.points)\n",
    "\n",
    "# Create a Delaunay triangulation\n",
    "tri = Delaunay(points[:, :2])\n",
    "\n",
    "# Define the function to integrate\n",
    "def integrand(x, y, tri, points):\n",
    "    simplex = tri.find_simplex(np.array([x, y]))\n",
    "    if simplex == -1:\n",
    "        return 0\n",
    "    vertices = points[tri.simplices[simplex]]\n",
    "    z = np.mean(vertices[:, 2])\n",
    "    return z\n",
    "\n",
    "# Define the bounds of the integration\n",
    "x_min, y_min = np.min(points[:, :2], axis=0)\n",
    "x_max, y_max = np.max(points[:, :2], axis=0)\n",
    "\n",
    "# Perform the double integral\n",
    "volume, error = dblquad(integrand, x_min, x_max, lambda x: y_min, lambda x: y_max, args=(tri, points))\n",
    "\n",
    "print(f\"Estimated volume: {volume}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte-Carlo method for volume estimation\n",
    "def monte_carlo_volume(points, num_samples=100000):\n",
    "    x_min, y_min, z_min = np.min(points, axis=0)\n",
    "    x_max, y_max, z_max = np.max(points, axis=0)\n",
    "    \n",
    "    samples = np.random.uniform([x_min, y_min, z_min], [x_max, y_max, z_max], (num_samples, 3))\n",
    "    delaunay_2d = Delaunay(points[:, :2])  # Create Delaunay triangulation using only x and y coordinates\n",
    "    inside_count = np.sum(delaunay_2d.find_simplex(samples[:, :2]) >= 0)\n",
    "    \n",
    "    bounding_volume = (x_max - x_min) * (y_max - y_min) * (z_max - z_min)\n",
    "    volume_monte_carlo = bounding_volume * (inside_count / num_samples)\n",
    "    \n",
    "    return volume_monte_carlo\n",
    "\n",
    "volume_monte_carlo = monte_carlo_volume(points)\n",
    "print(f\"Estimated volume using Monte-Carlo method: {volume_monte_carlo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f40a57b",
   "metadata": {},
   "source": [
    "### Задание 3. Анализ\n",
    "\n",
    "В чем, по Вашему мнению, заключается основная трудность при нахождении объемов объектов по 3D реконструкциям?\n",
    "\n",
    "Как можно улучшить получаемые результаты?\n",
    "\n",
    "*ВАШ КОММЕНТАРИЙ ЗДЕСЬ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bf6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код. Задание 4*. \n",
    "# Выполняйте это задание только после того, как выполните все остальные задания\n",
    "# Попробуйте реализовать предлагаемый Вами подход здесь\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c9709-0eca-4f94-9928-161f3d42d2a4",
   "metadata": {},
   "source": [
    "## Шаг 3: Включение неопределенности и теории вероятностей\n",
    "\n",
    "Реконструкции реального мира содержат неопределенности. Чтобы учесть это, мы сопоставляем значение неопределенности с каждой точкой в нашем облаке. \n",
    "\n",
    "Здесь мы присваиваем каждой точке случайное значение неопределенности и используем цветовую карту, чтобы визуализировать, где реконструкция может быть менее или более надежной."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142a147",
   "metadata": {},
   "source": [
    "В целом, расчёт интеграла очень часто встречается в теории вероятности, например, плотность вероятности случайной величины - это и есть интеграл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b8532-215f-4c59-81a4-cb9a2a8dfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an uncertainty value (between 0 and 1) for each point\n",
    "uncertainty = np.random.rand(len(points))\n",
    "\n",
    "# Create a color map based on uncertainty (using the viridis colormap)\n",
    "colors = plt.cm.viridis(uncertainty)[:, :3]\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# Visualize the point cloud with uncertainty colors\n",
    "o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "print(\"Uncertainty mapping applied to the point cloud.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d0f95",
   "metadata": {},
   "source": [
    "Теперь посмотрим как ещё могут быть применены неопределённости к расчёту объема общей неопределённости в реконструированном объекте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.integrate import nquad\n",
    "\n",
    "# Generate synthetic 3D points with uncertainties (covariance matrices)\n",
    "np.random.seed(42)\n",
    "n_points = 100\n",
    "points = np.random.randn(n_points, 3)  # 3D points (mean positions)\n",
    "\n",
    "# Define uncertainties (covariance matrices for each point)\n",
    "# Example: Diagonal covariance (variances for x, y, z)\n",
    "uncertainties = np.abs(np.random.rand(n_points, 3)) * 0.5  # Shape: (n_points, 3)\n",
    "\n",
    "# Define a 3D region of interest [a, b] × [c, d] × [e, f]\n",
    "region = [[-1, 1], [-1, 1], [-1, 1]]  # Cuboid from (-1, -1, -1) to (1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227eca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Method 1: Monte Carlo Integration (Scalable for 3D)\n",
    "# --------------------------------------------------------------------------------\n",
    "def monte_carlo_probability(points, uncertainties, region, n_samples=10_000):\n",
    "    # Randomly sample points from the Gaussian mixture\n",
    "    samples = []\n",
    "    for i in range(n_points):\n",
    "        mean = points[i]\n",
    "        cov = np.diag(uncertainties[i])  # Diagonal covariance matrix\n",
    "        samples.append(multivariate_normal.rvs(mean=mean, cov=cov, size=n_samples // n_points))\n",
    "    samples = np.vstack(samples)\n",
    "    \n",
    "    # Check if samples are within the region\n",
    "    in_region = (\n",
    "        (samples[:, 0] >= region[0][0]) & (samples[:, 0] <= region[0][1]) &\n",
    "        (samples[:, 1] >= region[1][0]) & (samples[:, 1] <= region[1][1]) &\n",
    "        (samples[:, 2] >= region[2][0]) & (samples[:, 2] <= region[2][1])\n",
    "    )\n",
    "    return np.mean(in_region)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Method 2: Direct Integration (Accurate but Slow for Large Datasets)\n",
    "# --------------------------------------------------------------------------------\n",
    "def gaussian_mixture_pdf(x, y, z, points, uncertainties):\n",
    "    pdf = 0.0\n",
    "    for i in range(len(points)):\n",
    "        mean = points[i]\n",
    "        cov = np.diag(uncertainties[i])\n",
    "        rv = multivariate_normal(mean=mean, cov=cov)\n",
    "        pdf += rv.pdf([x, y, z])\n",
    "    return pdf / len(points)  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если выполняется больше 3 минут, то в коде ошибка, можете переходить к следующим заданиям или попробовать исправить код\n",
    "# Integrate over the region\n",
    "integral, _ = nquad(\n",
    "    gaussian_mixture_pdf,\n",
    "    ranges=region,\n",
    "    args=(points, uncertainties),\n",
    "    opts={'limit': 2}  # Increase for accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Compare Results\n",
    "# --------------------------------------------------------------------------------\n",
    "# Empirical probability (fraction of original points in the region)\n",
    "empirical_prob = np.mean(\n",
    "    (points[:, 0] >= region[0][0]) & (points[:, 0] <= region[0][1]) &\n",
    "    (points[:, 1] >= region[1][0]) & (points[:, 1] <= region[1][1]) &\n",
    "    (points[:, 2] >= region[2][0]) & (points[:, 2] <= region[2][1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a276ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo estimate\n",
    "mc_prob = monte_carlo_probability(points, uncertainties, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Empirical Probability: {empirical_prob:.4f}\")\n",
    "print(f\"Monte Carlo Estimate: {mc_prob:.4f}\")\n",
    "print(f\"Direct Integration Estimate: {integral:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a8f32",
   "metadata": {},
   "source": [
    "Этот подход использует интегральное исчисление для количественной оценки неопределенности в трехмерных облаках точек, что делает его незаменимым для робототехники, лидарных данных и пространственной статистики."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d27356",
   "metadata": {},
   "source": [
    "### Задание 5. Анализ\n",
    "\n",
    "Как, используя полученные результаты, можно улучшить качество 3D-реконструкции собора? Какие трудности Вы ожидаете? Как бы Вы предложили их решить?\n",
    "\n",
    "*ВАШ КОММЕНТАРИЙ ЗДЕСЬ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код. Задание 6*. \n",
    "# Выполняйте это задание только после того, как выполните все остальные задания\n",
    "# Попробуйте реализовать предлагаемый Вами подход здесь\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2feb9-ace8-4507-8b77-73b70b32a480",
   "metadata": {},
   "source": [
    "## Шаг 4: Интегрирование и машинное обучение\n",
    "\n",
    "Недостающие или неопределенные данные в модели могут быть уточнены с помощью машинного обучения. В этой ячейке мы моделируем регрессионный эксперимент, который предсказывает \"глубину\" (или другую непрерывную переменную) на основе 2D-объектов. \n",
    "\n",
    "Представьте, что эти объекты получены в результате обработки изображений. Здесь мы:\n",
    "\n",
    "- Создаем синтетические 2D-точки данных\n",
    "- Определяем истинную взаимосвязь (функцию от x и y)\n",
    "- Намеренно маскируем некоторые данные, чтобы имитировать пробелы\n",
    "- Обучаем модель линейной регрессии для прогнозирования глубины\n",
    "- Сравниваем прогнозы с истинными значениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246df82-76ab-4a3c-8f03-61a14e70b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Simulate 100 random 2D feature coordinates (e.g., x, y from the image)\n",
    "X = np.random.rand(100, 2)\n",
    "\n",
    "# Define a true depth function, for instance: depth = sin(pi*x) * cos(pi*y)\n",
    "def true_depth(x, y):\n",
    "    return np.sin(np.pi * x) * np.cos(np.pi * y)\n",
    "\n",
    "y_true = np.array([true_depth(pt[0], pt[1]) for pt in X])\n",
    "\n",
    "# Introduce missing values in about 30% of the data to simulate measurement gaps\n",
    "mask = np.random.choice([True, False], size=y_true.shape, p=[0.7, 0.3])\n",
    "X_train = X[mask]\n",
    "y_train = y_true[mask]\n",
    "\n",
    "# Train a simple linear regression model using the available data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the full dataset\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot the true vs. predicted depths\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.7, color='teal')\n",
    "plt.xlabel(\"True Depth\")\n",
    "plt.ylabel(\"Predicted Depth\")\n",
    "plt.title(\"Regression: True vs. Predicted Depth\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Regression model trained and predictions generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9eb25",
   "metadata": {},
   "source": [
    "### Задание 7. Анализ\n",
    "\n",
    "Как можно применить полученные результаты к реальным данным из предыдущих раздело?\n",
    "\n",
    "*ВАШ КОММЕНТАРИЙ ЗДЕСЬ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b773624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код. Задание 8*. \n",
    "# Выполняйте это задание только после того, как выполните все остальные задания\n",
    "# Попробуйте реализовать предлагаемый Вами подход здесь\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Generate Synthetic Point Cloud Data\n",
    "# ---------------------------\n",
    "# (For illustration, we simulate the house's facade as a set of 2D points.)\n",
    "np.random.seed(42)\n",
    "true_mu_x = 10.0     # True x-center of the house\n",
    "true_mu_y = 15.0     # True y-center of the house\n",
    "sigma_x = 1.0        # Noise (measurement error) in x\n",
    "sigma_y = 1.5        # Noise in y\n",
    "N_points = 100       # Number of points in the cloud\n",
    "\n",
    "# Each coordinate is generated from a Gaussian distribution centered at the true value.\n",
    "x_data = np.random.normal(true_mu_x, sigma_x, N_points)\n",
    "y_data = np.random.normal(true_mu_y, sigma_y, N_points)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define a Bayesian Model for the House Center\n",
    "# ---------------------------\n",
    "# We assume that the observed data in each dimension is generated by:\n",
    "#   x_i ~ N(mu_x, sigma_x)   and   y_i ~ N(mu_y, sigma_y)\n",
    "# with a prior,\n",
    "#   mu_x ~ N(prior_mu_x, tau_x)   and   mu_y ~ N(prior_mu_y, tau_y)\n",
    "# Our goal is to compute the posterior p(mu | data) for each coordinate.\n",
    "#\n",
    "# In Bayesian inference, we have:\n",
    "#   p(mu | data) ∝ p(data | mu) * p(mu)\n",
    "#\n",
    "# Since p(data | mu) = ∏ᵢ N(data_i | mu, sigma)\n",
    "# and both likelihood and prior are continuous,\n",
    "# the normalization constant (marginal likelihood) is\n",
    "#   p(data) = ∫ p(data | mu) p(mu) dmu\n",
    "# Numerical integration (our \"integral\") is used to compute this constant.\n",
    "\n",
    "# Prior parameters for mu_x and mu_y:\n",
    "prior_mu_x = 8.0    # Our prior guess for the x-center\n",
    "tau_x = 2.0         # Prior standard deviation for mu_x\n",
    "prior_mu_y = 13.0   # Prior guess for the y-center\n",
    "tau_y = 2.0         # Prior standard deviation for mu_y\n",
    "\n",
    "# Define the log of the unnormalized posterior for one coordinate.\n",
    "def log_unnormalized_posterior(mu, data, sigma, prior_mu, tau):\n",
    "    # log_likelihood = ∑ log [N(data_i | mu, sigma)]\n",
    "    log_likelihood = np.sum(norm.logpdf(data, loc=mu, scale=sigma))\n",
    "    # log_prior = log [N(mu | prior_mu, tau)]\n",
    "    log_prior = norm.logpdf(mu, loc=prior_mu, scale=tau)\n",
    "    return log_likelihood + log_prior\n",
    "\n",
    "# For numerical stability, we compute on a grid first to determine a scaling constant.\n",
    "def compute_scaling_constant(data, sigma, prior_mu, tau, grid_min, grid_max, num_points=1000):\n",
    "    mu_grid = np.linspace(grid_min, grid_max, num_points)\n",
    "    log_vals = [log_unnormalized_posterior(mu, data, sigma, prior_mu, tau) for mu in mu_grid]\n",
    "    return np.max(log_vals)\n",
    "\n",
    "# Define a function for the unnormalized posterior (in standard space).\n",
    "def unnormalized_posterior(mu, data, sigma, prior_mu, tau, c):\n",
    "    # We subtract a constant c from the log posterior to avoid numerical underflow.\n",
    "    return np.exp(log_unnormalized_posterior(mu, data, sigma, prior_mu, tau) - c)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Inference for the x-coordinate (mu_x)\n",
    "# ---------------------------\n",
    "# Define a grid where we expect mu_x to lie.\n",
    "grid_min_x = prior_mu_x - 4 * tau_x\n",
    "grid_max_x = prior_mu_x + 4 * tau_x\n",
    "# Determine a scaling constant to keep values numerically stable.\n",
    "c_x = compute_scaling_constant(x_data, sigma_x, prior_mu_x, tau_x, grid_min_x, grid_max_x)\n",
    "\n",
    "# Compute the marginal likelihood by integrating the unnormalized posterior over mu.\n",
    "marginal_x, err_x = quad(lambda u: unnormalized_posterior(u, x_data, sigma_x, prior_mu_x, tau_x, c_x), -np.inf, np.inf)\n",
    "# The true marginal likelihood is: marginal_x * exp(c_x)\n",
    "\n",
    "# Define the normalized posterior for mu_x.\n",
    "def posterior_mu(mu, data, sigma, prior_mu, tau, c, marginal):\n",
    "    return np.exp(log_unnormalized_posterior(mu, data, sigma, prior_mu, tau) - c) / marginal\n",
    "\n",
    "# Evaluate the posterior over a grid.\n",
    "mu_x_values = np.linspace(grid_min_x, grid_max_x, 400)\n",
    "posterior_x_vals = [posterior_mu(mu, x_data, sigma_x, prior_mu_x, tau_x, c_x, marginal_x) for mu in mu_x_values]\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Inference for the y-coordinate (mu_y)\n",
    "# ---------------------------\n",
    "grid_min_y = prior_mu_y - 4 * tau_y\n",
    "grid_max_y = prior_mu_y + 4 * tau_y\n",
    "c_y = compute_scaling_constant(y_data, sigma_y, prior_mu_y, tau_y, grid_min_y, grid_max_y)\n",
    "marginal_y, err_y = quad(lambda u: unnormalized_posterior(u, y_data, sigma_y, prior_mu_y, tau_y, c_y), -np.inf, np.inf)\n",
    "mu_y_values = np.linspace(grid_min_y, grid_max_y, 400)\n",
    "posterior_y_vals = [posterior_mu(mu, y_data, sigma_y, prior_mu_y, tau_y, c_y, marginal_y) for mu in mu_y_values]\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Visualizations\n",
    "# ---------------------------\n",
    "# Plot the posterior distributions for mu_x and mu_y.\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mu_x_values, posterior_x_vals, label='Posterior p(mu_x|data)')\n",
    "plt.axvline(true_mu_x, color='r', linestyle='--', label='True mu_x')\n",
    "plt.xlabel('mu_x')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Posterior for House X-center')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(mu_y_values, posterior_y_vals, label='Posterior p(mu_y|data)')\n",
    "plt.axvline(true_mu_y, color='r', linestyle='--', label='True mu_y')\n",
    "plt.xlabel('mu_y')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Posterior for House Y-center')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Compute a conditional probability:\n",
    "# For example, what is the probability that mu_x > 9 given the observed data?\n",
    "cond_prob_x, err_cond_x = quad(lambda u: posterior_mu(u, x_data, sigma_x, prior_mu_x, tau_x, c_x, marginal_x), 9, np.inf)\n",
    "print(f\"Probability that house x-center mu_x > 9 given data: {cond_prob_x:.3f}\")\n",
    "\n",
    "# Visualize the synthetic 2D point cloud and the empirical (sample) mean.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_data, y_data, alpha=0.6, label='Point Cloud Data')\n",
    "plt.scatter(np.mean(x_data), np.mean(y_data), color='r', marker='x', s=100, label='Empirical Mean')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.title('Synthetic House Point Cloud')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145e23e",
   "metadata": {},
   "source": [
    "### Задание 9. Анализ\n",
    "\n",
    "Предложите способы применения подходов из предыдущей ячейки к реальным данным из предыдущего раздела.\n",
    "\n",
    "*ВАШ КОММЕНТАРИЙ ЗДЕСЬ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fbf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код. Задание 10*. \n",
    "# Выполняйте это задание только после того, как выполните все остальные задания\n",
    "# Попробуйте реализовать предлагаемый Вами подход здесь\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7f54c-6ec4-411f-b7cd-0cee255a3d79",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "В этом ноутбуке мы:\n",
    "\n",
    "- Обработали образцы изображений Собора Парижской Богоматери\n",
    "- Смоделировали облако точек на основе многовидовой (multi-view) реконструкции\n",
    "- Сопоставили неопределенность с нашей моделью, используя теорию вероятностей\n",
    "- Использовали базовую модель машинного обучения для расчёта условной вероятности.\n",
    "\n",
    "Это целостное исследование показывает, что интегральное исчисление — это не изолированная концепция, а связующее звено, которое объединяет обработку изображений, 3D-моделирование, количественную оценку неопределенности и машинное обучение при решении реальных задач.\n",
    "\n",
    "**Следующие шаги:**\n",
    "\n",
    "- Замените смоделированные компоненты реальными конвейерами SfM/3D реконструкции.\n",
    "- Поэкспериментируйте с различными подходами к моделированию для прогнозирования неопределенности и глубины.\n",
    "- Изучите другие методы численного интегрирования и уточните реконструкцию с помощью более сложных моделей машинного обучения.\n",
    "\n",
    "Приятного изучения!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316d641",
   "metadata": {},
   "source": [
    "### Дополнительные исследования\n",
    "\n",
    "Теперь, когда в ноутбуке изложено самое необходимое, подумайте о том, чтобы расширить ее в нескольких направлениях:\n",
    "\n",
    "- **Улучшенная 3D-реконструкция:** Интегрируйте соответствующий пакет \"Структура из движения\" (Structure-from-Motion package) (например, COLMAP) и обработайте несколько изображений, чтобы создать точную 3D-модель Собора Парижской Богоматери.  \n",
    "- **Усовершенствованное моделирование неопределенности:** Используйте байесовские методы для количественной оценки неопределенности реконструкции и распространения ее на ваши интегральные вычисления.  \n",
    "- **Подходы к глубокому обучению:** Использование сверточных нейронных сетей или алгоритмов заполнения глубины для улучшения прогнозирования недостающих данных о глубине, тем самым способствуя более точной интеграции объемов.  \n",
    "\n",
    "Этот ноутбук — не просто учебное пособие, это приглашение исследовать переплетенные миры математики и компьютерных наук, одновременно отдавая дань уважения историческому шедевру. Приятного путешествия!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949ce0e",
   "metadata": {},
   "source": [
    "### Задание 11. Анализ\n",
    "\n",
    "Проанализируйте проделанную Вами сегодня работу: в каких случаях применение явных методов интегрирования даёт преимущества, а в каких случаях следует использовать более хитрые модели расчёта интегралов для получения лучших результат?\n",
    "\n",
    "Что нового Вы лично для себя сегодня узнали?\n",
    "\n",
    "*ВАШ КОММЕНТАРИЙ ЗДЕСЬ*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
